{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "\n",
    "    theta = theta.reshape((k, n))\n",
    "\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "\n",
    "    grad = grad.ravel()\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    opt_theta = results['x']\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model\n",
    "\n",
    "def softmax_predict(model, data):\n",
    "    theta = model['opt_theta']\n",
    "    k = model['n_classes']\n",
    "    n = model['input_size']\n",
    "    theta = theta.reshape((k, n))\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def compute_numerical_gradient(J, theta):\n",
    "    n = theta.size\n",
    "    grad = np.zeros(n)\n",
    "    eps = 1.0e-4\n",
    "    eps2 = 2*eps\n",
    "    for i in range(n):\n",
    "        theta_p = theta.copy()\n",
    "        theta_n = theta.copy()\n",
    "        theta_p[i] = theta[i] + eps\n",
    "        theta_n[i] = theta[i] - eps\n",
    "        grad[i] = (J(theta_p) - J(theta_n)) / eps2\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_network(A):\n",
    "    opt_normalize = True\n",
    "    opt_graycolor = True\n",
    "\n",
    "    A = A - np.average(A)\n",
    "\n",
    "    (row, col) = A.shape\n",
    "    sz = int(np.ceil(np.sqrt(row)))\n",
    "    buf = 1\n",
    "    n = np.ceil(np.sqrt(col))\n",
    "    m = np.ceil(col / n)\n",
    "    \n",
    "    img_shape1 = int(buf + m * (sz + buf))\n",
    "    img_shape2 = int(buf + n * (sz + buf))\n",
    "    image = np.ones(shape=(img_shape1, img_shape2))\n",
    "\n",
    "    if not opt_graycolor:\n",
    "        image *= 0.1\n",
    "\n",
    "    k = 0\n",
    "    for i in range(int(m)):\n",
    "        for j in range(int(n)):\n",
    "            if k >= col:\n",
    "                continue\n",
    "            clim = np.max(np.abs(A[:, k]))\n",
    "            if opt_normalize:\n",
    "                image[buf + i * (sz + buf):buf + i * (sz + buf) + sz, buf + j * (sz + buf):buf + j * (sz + buf) + sz] = \\\n",
    "                    A[:, k].reshape(sz, sz) / clim\n",
    "            else:\n",
    "                image[buf + i * (sz + buf):buf + i * (sz + buf) + sz, buf + j * (sz + buf):buf + j * (sz + buf) + sz] = \\\n",
    "                    A[:, k].reshape(sz, sz) / np.max(np.abs(A))\n",
    "            k += 1\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def load_MNIST_images(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        n_images = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        rows = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        cols = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        images = np.fromfile(f, dtype=np.ubyte)\n",
    "        images = images.reshape((int(n_images), int(rows * cols)))\n",
    "        images = images.T\n",
    "        images = images.astype(np.float64) / 255\n",
    "        f.close()\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_MNIST_labels(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        n_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        f.close()\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(hidden_size, visible_size):\n",
    "    r  = np.sqrt(6) / np.sqrt(hidden_size + visible_size + 1)\n",
    "    W1 = np.random.random((hidden_size, visible_size)) * 2.0 * r - r\n",
    "    W2 = np.random.random((visible_size, hidden_size)) * 2.0 * r - r\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    b2 = np.zeros(visible_size)\n",
    "    theta = np.hstack((W1.ravel(), W2.ravel(), b1.ravel(), b2.ravel()))\n",
    "    return theta\n",
    "\n",
    "\n",
    "def sigmoid(z2):\n",
    "    return 1/(1 + np.exp(-1*z2))\n",
    "\n",
    "def sparse_autoencoder_cost(theta, visible_size, hidden_size, lambda_, sparsity_param, beta, data):\n",
    "    W1 = theta[:visible_size*hidden_size].reshape((hidden_size, visible_size))\n",
    "    W2 = theta[visible_size*hidden_size:2*hidden_size*visible_size].reshape((visible_size, hidden_size))\n",
    "    b1 = theta[2*hidden_size*visible_size:2*hidden_size*visible_size+hidden_size]\n",
    "    b2 = theta[2*hidden_size*visible_size+hidden_size:]\n",
    "    \n",
    "    m = data.shape[1]\n",
    "    \n",
    "    a1 = data\n",
    "    z2 = W1.dot(a1) + b1.reshape((-1, 1))\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = W2.dot(a2) + b2.reshape((-1, 1))\n",
    "    a3 = sigmoid(z3)\n",
    "    h = a3\n",
    "    y = a1\n",
    "    \n",
    "    rho = sparsity_param\n",
    "    rho_hat = np.mean(a2, axis = 1)\n",
    "    sparsity_delta = (-rho/rho_hat + (1.0 - rho)/(1.0 - rho_hat)).reshape((-1,1))\n",
    "    \n",
    "    delta3 = (h-y)*h*(1.0-h)\n",
    "    delta2 = (W2.T.dot(delta3) + beta*sparsity_delta)*a2*(1.0-a2)\n",
    "    \n",
    "    squared_error_term = np.sum((h-y)**2)/(2.0*m)\n",
    "    weight_decay = 0.5*lambda_*(np.sum(W1*W1) + np.sum(W2*W2))\n",
    "    sparsity_term = beta*np.sum(rho*np.log(rho/rho_hat) + (1.0-rho)*np.log((1.0-rho)/(1.0-rho_hat)))\n",
    "    cost = squared_error_term + weight_decay + sparsity_term\n",
    "    \n",
    "    W2_grad = delta3.dot(a2.T)/m + lambda_*W2\n",
    "    W1_grad = delta2.dot(a1.T)/m + lambda_*W1\n",
    "    b1_grad = np.mean(delta2, axis = 1)\n",
    "    b2_grad = np.mean(delta3, axis = 1)\n",
    "    grad = np.hstack((W1_grad.ravel(), W2_grad.ravel(), b1_grad, b2_grad))\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "\n",
    "def feedforward_autoencoder(theta, hidden_size, visible_size, data):\n",
    "    W1 = theta[0 : hidden_size*visible_size].reshape((hidden_size, visible_size))\n",
    "    b1 = theta[2*hidden_size*visible_size : 2*hidden_size*visible_size+hidden_size].reshape((-1, 1))\n",
    "    a1 = data\n",
    "    z2 = W1.dot(a1) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_ae_cost(theta, input_size, hidden_size,\n",
    "                    n_classes, net_config, lambda_, data, labels):\n",
    "   \n",
    "    softmax_theta = theta[0:hidden_size*n_classes].reshape((n_classes, hidden_size))\n",
    "\n",
    "    # Extract out the \"stack\"\n",
    "    stack = params2stack(theta[hidden_size*n_classes:], net_config)\n",
    "\n",
    "    # Number of examples\n",
    "    m = data.shape[1]\n",
    "\n",
    "    # Forword pass\n",
    "    z = [np.zeros(1)] # Note that z[0] is dummy\n",
    "    a = [data]\n",
    "    for s in stack:\n",
    "        z.append(s['w'].dot(a[-1]) + s['b'].reshape((-1, 1)) )\n",
    "        a.append(sigmoid(z[-1]))\n",
    "\n",
    "    learned_features = a[-1]\n",
    "\n",
    "    # Probability with shape (n_classes, m)\n",
    "    theta_features = softmax_theta.dot(learned_features)\n",
    "    alpha = np.max(theta_features, axis=0)\n",
    "    theta_features -= alpha # Avoid numerical problem due to large values of exp(theta_features)\n",
    "    proba = np.exp(theta_features) / np.sum(np.exp(theta_features), axis=0)\n",
    "\n",
    "    # Matrix of indicator fuction with shape (n_classes, m)\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "\n",
    "    # Compute softmax cost and gradient\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(softmax_theta*softmax_theta)\n",
    "    softmax_grad = -1.0/m * (indicator - proba).dot(learned_features.T) + lambda_*softmax_theta\n",
    "\n",
    "    # Backpropagation\n",
    "    delta = [- softmax_theta.T.dot(indicator - proba) * sigmoid_prime(z[-1])]\n",
    "    n_stack = len(stack)\n",
    "    for i in reversed(range(n_stack)): # Note that delta[0] will not be used\n",
    "        d = stack[i]['w'].T.dot(delta[0])*sigmoid_prime(z[i])\n",
    "        delta.insert(0, d) # Insert element at beginning\n",
    "\n",
    "    stack_grad = [{} for i in range(n_stack)]\n",
    "    for i in range(n_stack):\n",
    "        stack_grad[i]['w'] = delta[i+1].dot(a[i].T) / m\n",
    "        stack_grad[i]['b'] = np.mean(delta[i+1], axis=1)\n",
    "\n",
    "    stack_grad_params = stack2params(stack_grad)[0]\n",
    "\n",
    "    grad = np.concatenate((softmax_grad.flatten(), stack_grad_params))\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def stacked_ae_predict(theta, input_size, hidden_size,\n",
    "                       n_classes, net_config, data):\n",
    "  \n",
    "\n",
    "    # We first extract the part which compute the softmax gradient\n",
    "    softmax_theta = theta[0:hidden_size*n_classes].reshape((n_classes, hidden_size))\n",
    "\n",
    "    # Extract out the \"stack\"\n",
    "    stack = params2stack(theta[hidden_size*n_classes:], net_config)\n",
    "\n",
    "    # Number of examples\n",
    "    m = data.shape[1]\n",
    "\n",
    "    # Forword pass\n",
    "    z = [np.zeros(1)]\n",
    "    a = [data]\n",
    "    for s in stack:\n",
    "        z.append(s['w'].dot(a[-1]) + s['b'].reshape((-1, 1)) )\n",
    "        a.append(sigmoid(z[-1]))\n",
    "\n",
    "    learned_features = a[-1]\n",
    "\n",
    "    # Softmax model\n",
    "    model = {}\n",
    "    model['opt_theta']  = softmax_theta\n",
    "    model['n_classes']  = n_classes\n",
    "    model['input_size'] = hidden_size\n",
    "\n",
    "    # Make predictions\n",
    "    pred = softmax_predict(model, learned_features)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def check_stacked_ae_cost():\n",
    "    \"\"\"\n",
    "    Check the gradients for the stacked autoencoder.\n",
    "\n",
    "    In general, we recommend that the creation of such files for checking\n",
    "    gradients when you write new cost functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup random data / small model\n",
    "    input_size = 4;\n",
    "    hidden_size = 5;\n",
    "    lambda_ = 0.01;\n",
    "    data   = np.random.randn(input_size, 5)\n",
    "    labels = np.array([ 0, 1, 0, 1, 0], dtype=np.uint8)\n",
    "    n_classes = 2\n",
    "    n_stack = 2\n",
    "\n",
    "    stack = [{} for i in range(n_stack)]\n",
    "    stack[0]['w'] = 0.1 * np.random.randn(3, input_size)\n",
    "    stack[0]['b'] = np.zeros(3)\n",
    "    stack[1]['w'] = 0.1 * np.random.randn(hidden_size, 3)\n",
    "    stack[1]['b'] = np.zeros(hidden_size)\n",
    "\n",
    "    softmax_theta = 0.005 * np.random.randn(hidden_size * n_classes)\n",
    "\n",
    "    stack_params, net_config = stack2params(stack)\n",
    "    stacked_ae_theta = np.concatenate((softmax_theta, stack_params))\n",
    "\n",
    "    cost, grad = stacked_ae_cost(stacked_ae_theta, input_size, hidden_size,\n",
    "                                 n_classes, net_config, lambda_, data, labels)\n",
    "\n",
    "    # Check that the numerical and analytic gradients are the same\n",
    "    J = lambda theta : stacked_ae_cost(theta, input_size, hidden_size,\n",
    "                                       n_classes, net_config, lambda_, data, labels)[0]\n",
    "    nume_grad = compute_numerical_gradient(J, stacked_ae_theta)\n",
    "\n",
    "    # Use this to visually compare the gradients side by side\n",
    "    for i in range(grad.size):\n",
    "        print(\"{0:20.12f} {1:20.12f}\".format(nume_grad[i], grad[i]))\n",
    "    print('The above two columns you get should be very similar.\\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n')\n",
    "\n",
    "    # Compare numerically computed gradients with the ones obtained from backpropagation\n",
    "    # The difference should be small. In our implementation, these values are usually less than 1e-9.\n",
    "    # When you got this working, Congratulations!!!\n",
    "    diff = np.linalg.norm(nume_grad - grad) / np.linalg.norm(nume_grad + grad)\n",
    "    print(\"Norm of difference = \", diff)\n",
    "    print('Norm of the difference between numerical and analytical gradient (should be < 1e-9)\\n\\n')\n",
    "\n",
    "\n",
    "def stack2params(stack):\n",
    "    \"\"\"\n",
    "    Converts a \"stack\" structure into a flattened parameter vector and also\n",
    "    stores the network configuration.\n",
    "\n",
    "    stack: the stack structure, where\n",
    "           stack[0]['w'] = weights of first layer\n",
    "           stack[0]['b'] = weights of first layer\n",
    "           stack[1]['w'] = weights of second layer\n",
    "           stack[1]['b'] = weights of second layer\n",
    "                                           ... etc.\n",
    "    params: parameter vector.\n",
    "    net_config: configuration of network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup the compressed param vector\n",
    "    params = []\n",
    "    for i in range(len(stack)):\n",
    "        w = stack[i]['w']\n",
    "        b = stack[i]['b']\n",
    "        params.append(w.flatten())\n",
    "        params.append(b.flatten())\n",
    "\n",
    "        # Check that stack is of the correct form\n",
    "        assert w.shape[0] == b.size, \\\n",
    "            'The size of bias should equals to the column size of W for layer {}'.format(i)\n",
    "        if i < len(stack)-1:\n",
    "            assert stack[i]['w'].shape[0] == stack[i+1]['w'].shape[1], \\\n",
    "                'The adjacent layers L {} and L {} should have matching sizes.'.format(i, i+1)\n",
    "\n",
    "    params = np.concatenate(params)\n",
    "\n",
    "    # Setup network configuration\n",
    "    net_config = {}\n",
    "    if len(stack) == 0:\n",
    "        net_config['input_size'] = 0\n",
    "        net_config['layer_sizes'] = []\n",
    "    else:\n",
    "        net_config['input_size'] = stack[0]['w'].shape[1]\n",
    "        net_config['layer_sizes'] = []\n",
    "        for s in stack:\n",
    "            net_config['layer_sizes'].append(s['w'].shape[0])\n",
    "\n",
    "    return params, net_config\n",
    "\n",
    "\n",
    "def params2stack(params, net_config):\n",
    "    \"\"\"\n",
    "    Converts a flattened parameter vector into a nice \"stack\" structure\n",
    "    for us to work with. This is useful when you're building multilayer\n",
    "    networks.\n",
    "\n",
    "    params: flattened parameter vector\n",
    "    net_config: auxiliary variable containing the configuration of the network\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the params (a vector into a stack of weights)\n",
    "    layer_sizes = net_config['layer_sizes']\n",
    "    prev_layer_size = net_config['input_size'] # the size of the previous layer\n",
    "    depth = len(layer_sizes)\n",
    "    stack = [{} for i in range(depth)]\n",
    "    current_pos = 0                           # mark current position in parameter vector\n",
    "\n",
    "    for i in range(depth):\n",
    "        # Extract weights\n",
    "        wlen = layer_sizes[i] * prev_layer_size\n",
    "        stack[i]['w'] = params[current_pos:current_pos+wlen].reshape((layer_sizes[i], prev_layer_size))\n",
    "        current_pos += wlen\n",
    "\n",
    "        # Extract bias\n",
    "        blen = layer_sizes[i]\n",
    "        stack[i]['b'] = params[current_pos:current_pos+blen]\n",
    "        current_pos += blen\n",
    "\n",
    "        # Set previous layer size\n",
    "        prev_layer_size = layer_sizes[i]\n",
    "\n",
    "    return stack\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "STEP 0: Here we provide the relevant parameters values that will\n",
    "  allow your sparse autoencoder to get good filters; you do not need to\n",
    "  change the parameters below.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_size = 28 * 28\n",
    "n_classes = 10         # Number of classes\n",
    "hidden_size_L1 = 200   # Layer 1 Hidden Size\n",
    "hidden_size_L2 = 200   # Layer 2 Hidden Size\n",
    "sparsity_param = 0.1   # desired average activation of the hidden units.\n",
    "                       # (This was denoted by the Greek alphabet rho, which looks like a lower-case \"p\",\n",
    "                       #  in the lecture notes).\n",
    "lambda_ = 3e-3         # weight decay parameter\n",
    "beta = 3               # weight of sparsity penalty term\n",
    "\n",
    "maxiter = 400          # Maximum iterations for training\n",
    "\n",
    "\"\"\"\n",
    "STEP 1: Load data from the MNIST database\n",
    "\n",
    "  This loads our training data from the MNIST database files.\n",
    "\"\"\"\n",
    "\n",
    "# Load MNIST database files\n",
    "# Load MNIST database files\n",
    "train_data   = load_MNIST_images('train-images-idx3-ubyte')\n",
    "train_labels = load_MNIST_labels('train-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STEP 2: Train the first sparse autoencoder\n",
    "\n",
    "  This trains the first sparse autoencoder on the unlabelled STL training images.\n",
    "  If you've correctly implemented sparse_autoencoder_cost, you don't need\n",
    "  to change anything here.\n",
    "\"\"\"\n",
    "\n",
    "# Randomly initialize the parameters\n",
    "sae1_theta = initialize_parameters(hidden_size_L1, input_size)\n",
    "\n",
    "#  Instructions: Train the first layer sparse autoencoder, this layer has\n",
    "#                an hidden size of \"hidden_size_L1\"\n",
    "#                You should store the optimal parameters in sae1_opt_theta\n",
    "\n",
    "J = lambda theta : sparse_autoencoder_cost(theta, input_size, hidden_size_L1, lambda_, sparsity_param, beta, train_data)\n",
    "\n",
    "options = {'maxiter': maxiter, 'disp': True}\n",
    "\n",
    "results = scipy.optimize.minimize(J, sae1_theta, method='L-BFGS-B', jac=True, options=options)\n",
    "sae1_opt_theta = results['x']\n",
    "\n",
    "print(\"Show the results of optimization as following.\\n\")\n",
    "print(results)\n",
    "\n",
    "# Visualize weights\n",
    "visualize_weights = False\n",
    "if visualize_weights:\n",
    "    W1 = sae1_opt_theta[0:hidden_size_L1*input_size].reshape((hidden_size_L1, input_size))\n",
    "    image = display_network(W1.T)\n",
    "    plt.figure()\n",
    "    plt.imshow(image, cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STEP 2: Train the second sparse autoencoder\n",
    "\n",
    "  This trains the second sparse autoencoder on the first autoencoder featurse.\n",
    "  If you've correctly implemented sparse_autoencoder_cost, you don't need\n",
    "  to change anything here.\n",
    "\"\"\"\n",
    "\n",
    "sae1_features = feedforward_autoencoder(sae1_opt_theta, hidden_size_L1, input_size, train_data)\n",
    "\n",
    "#  Randomly initialize the parameters\n",
    "sae2_theta = initialize_parameters(hidden_size_L2, hidden_size_L1)\n",
    "\n",
    "#  Instructions: Train the second layer sparse autoencoder, this layer has\n",
    "#                an hidden size of \"hidden_size_L2\" and an input size of \"hidden_size_L1\"\n",
    "#                You should store the optimal parameters in sae2_opt_theta\n",
    "J = lambda theta : sparse_autoencoder_cost(theta, hidden_size_L1, hidden_size_L2,\n",
    "    lambda_, sparsity_param, beta, sae1_features)\n",
    "\n",
    "options = {'maxiter': maxiter, 'disp': True}\n",
    "\n",
    "results = scipy.optimize.minimize(J, sae2_theta, method='L-BFGS-B', jac=True, options=options)\n",
    "sae2_opt_theta = results['x']\n",
    "\n",
    "print(\"Show the results of optimization as following.\\n\")\n",
    "print(results)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STEP 3: Train the softmax classifier\n",
    "\n",
    "  This trains the sparse autoencoder on the second autoencoder features.\n",
    "  If you've correctly implemented softmax_cost, you don't need\n",
    "  to change anything here.\n",
    "\"\"\"\n",
    "\n",
    "sae2_features = feedforward_autoencoder(sae2_opt_theta, hidden_size_L2, hidden_size_L1, sae1_features)\n",
    "\n",
    "#  Instructions: Train the softmax classifier, the classifier takes in\n",
    "#                input of dimension \"hidden_sizeL2\" corresponding to the\n",
    "#                hidden layer size of the 2nd layer.\n",
    "#\n",
    "#                You should store the optimal parameters in sae_softmax_opt_theta\n",
    "\n",
    "\n",
    "options = {'maxiter': maxiter, 'disp': True}\n",
    "softmax_model = softmax_train(hidden_size_L2, n_classes, lambda_, sae2_features, train_labels, options)\n",
    "softmax_opt_theta = softmax_model['opt_theta']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STEP 5: Finetune softmax model\n",
    "\"\"\"\n",
    "\n",
    "# Implement the stacked_ae_cost to give the combined cost of the whole model then run this cell.\n",
    "\n",
    "# Initialize the stack using the parameters learned\n",
    "\n",
    "n_stack = 2 # Two layers\n",
    "stack = [{} for i in range(n_stack)]\n",
    "\n",
    "stack[0]['w'] = sae1_opt_theta[0:hidden_size_L1*input_size].reshape((hidden_size_L1, input_size))\n",
    "stack[0]['b'] = sae1_opt_theta[2*hidden_size_L1*input_size: 2*hidden_size_L1*input_size + hidden_size_L1]\n",
    "\n",
    "stack[1]['w'] = sae2_opt_theta[0:hidden_size_L2*hidden_size_L1].reshape((hidden_size_L2, hidden_size_L1))\n",
    "stack[1]['b'] = sae2_opt_theta[2*hidden_size_L2*hidden_size_L1: 2*hidden_size_L2*hidden_size_L1 + hidden_size_L2]\n",
    "\n",
    "# Initialize the parameters for the deep model\n",
    "stack_params, net_config = stack2params(stack)\n",
    "stacked_ae_theta = np.concatenate((softmax_opt_theta, stack_params))\n",
    "\n",
    "# Instructions: Train the deep network, hidden size here refers to the\n",
    "#               dimension of the input to the classifier, which corresponds\n",
    "#               to \"hidden_size_L2\".\n",
    "\n",
    "J = lambda theta : stacked_ae_cost(theta, input_size, hidden_size_L2, n_classes, net_config, lambda_, train_data, train_labels)\n",
    "\n",
    "#check_stacked_ae_cost() # Verify the correctness\n",
    "\n",
    "# Find out the optimal theta\n",
    "options = {'maxiter': maxiter, 'disp': True}\n",
    "results = scipy.optimize.minimize(J, stacked_ae_theta, method='L-BFGS-B', jac=True, options=options)\n",
    "stacked_ae_opt_theta = results['x']\n",
    "\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "STEP 6: Test\n",
    "  Instructions: You will need to complete the code in stacked_ae_predict\n",
    "                before running this part of the code\n",
    "\"\"\"\n",
    "\n",
    "# Get labelled test images\n",
    "# Note that we apply the same kind of preprocessing as the training set\n",
    "test_data   = load_MNIST_images('t10k-images-idx3-ubyte')\n",
    "test_labels = load_MNIST_labels('t10k-labels-idx1-ubyte')\n",
    "\n",
    "pred = stacked_ae_predict(stacked_ae_theta, input_size, hidden_size_L2, n_classes, net_config, test_data)\n",
    "\n",
    "acc = np.mean(test_labels == pred)\n",
    "print(\"Before Finetuning Test Accuracy: {:5.2f}% \\n\".format(acc*100))\n",
    "\n",
    "pred = stacked_ae_predict(stacked_ae_opt_theta, input_size, hidden_size_L2, n_classes, net_config, test_data)\n",
    "\n",
    "acc = np.mean(test_labels == pred)\n",
    "print(\"After Finetuning Test Accuracy: {:5.2f}% \\n\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
