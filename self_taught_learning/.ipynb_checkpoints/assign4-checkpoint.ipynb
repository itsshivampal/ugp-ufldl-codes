{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_network(A):\n",
    "    opt_normalize = True\n",
    "    opt_graycolor = True\n",
    "\n",
    "    A = A - np.average(A)\n",
    "\n",
    "    (row, col) = A.shape\n",
    "    sz = int(np.ceil(np.sqrt(row)))\n",
    "    buf = 1\n",
    "    n = np.ceil(np.sqrt(col))\n",
    "    m = np.ceil(col / n)\n",
    "    \n",
    "    img_shape1 = int(buf + m * (sz + buf))\n",
    "    img_shape2 = int(buf + n * (sz + buf))\n",
    "    image = np.ones(shape=(img_shape1, img_shape2))\n",
    "\n",
    "    if not opt_graycolor:\n",
    "        image *= 0.1\n",
    "\n",
    "    k = 0\n",
    "    for i in range(int(m)):\n",
    "        for j in range(int(n)):\n",
    "            if k >= col:\n",
    "                continue\n",
    "            clim = np.max(np.abs(A[:, k]))\n",
    "            if opt_normalize:\n",
    "                image[buf + i * (sz + buf):buf + i * (sz + buf) + sz, buf + j * (sz + buf):buf + j * (sz + buf) + sz] = \\\n",
    "                    A[:, k].reshape(sz, sz) / clim\n",
    "            else:\n",
    "                image[buf + i * (sz + buf):buf + i * (sz + buf) + sz, buf + j * (sz + buf):buf + j * (sz + buf) + sz] = \\\n",
    "                    A[:, k].reshape(sz, sz) / np.max(np.abs(A))\n",
    "            k += 1\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def load_MNIST_images(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        n_images = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        rows = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        cols = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        images = np.fromfile(f, dtype=np.ubyte)\n",
    "        images = images.reshape((int(n_images), int(rows * cols)))\n",
    "        images = images.T\n",
    "        images = images.astype(np.float64) / 255\n",
    "        f.close()\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_MNIST_labels(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        n_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        f.close()\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(hidden_size, visible_size):\n",
    "    r  = np.sqrt(6) / np.sqrt(hidden_size + visible_size + 1)\n",
    "    W1 = np.random.random((hidden_size, visible_size)) * 2.0 * r - r\n",
    "    W2 = np.random.random((visible_size, hidden_size)) * 2.0 * r - r\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    b2 = np.zeros(visible_size)\n",
    "    theta = np.hstack((W1.ravel(), W2.ravel(), b1.ravel(), b2.ravel()))\n",
    "    return theta\n",
    "\n",
    "\n",
    "def sigmoid(z2):\n",
    "    return 1/(1 + np.exp(-1*z2))\n",
    "\n",
    "def sparse_autoencoder_cost(theta, visible_size, hidden_size, lambda_, sparsity_param, beta, data):\n",
    "    W1 = theta[:visible_size*hidden_size].reshape((hidden_size, visible_size))\n",
    "    W2 = theta[visible_size*hidden_size:2*hidden_size*visible_size].reshape((visible_size, hidden_size))\n",
    "    b1 = theta[2*hidden_size*visible_size:2*hidden_size*visible_size+hidden_size]\n",
    "    b2 = theta[2*hidden_size*visible_size+hidden_size:]\n",
    "    \n",
    "    m = data.shape[1]\n",
    "    \n",
    "    a1 = data\n",
    "    z2 = W1.dot(a1) + b1.reshape((-1, 1))\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = W2.dot(a2) + b2.reshape((-1, 1))\n",
    "    a3 = sigmoid(z3)\n",
    "    h = a3\n",
    "    y = a1\n",
    "    \n",
    "    rho = sparsity_param\n",
    "    rho_hat = np.mean(a2, axis = 1)\n",
    "    sparsity_delta = (-rho/rho_hat + (1.0 - rho)/(1.0 - rho_hat)).reshape((-1,1))\n",
    "    \n",
    "    delta3 = (h-y)*h*(1.0-h)\n",
    "    delta2 = (W2.T.dot(delta3) + beta*sparsity_delta)*a2*(1.0-a2)\n",
    "    \n",
    "    squared_error_term = np.sum((h-y)**2)/(2.0*m)\n",
    "    weight_decay = 0.5*lambda_*(np.sum(W1*W1) + np.sum(W2*W2))\n",
    "    sparsity_term = beta*np.sum(rho*np.log(rho/rho_hat) + (1.0-rho)*np.log((1.0-rho)/(1.0-rho_hat)))\n",
    "    cost = squared_error_term + weight_decay + sparsity_term\n",
    "    \n",
    "    W2_grad = delta3.dot(a2.T)/m + lambda_*W2\n",
    "    W1_grad = delta2.dot(a1.T)/m + lambda_*W1\n",
    "    b1_grad = np.mean(delta2, axis = 1)\n",
    "    b2_grad = np.mean(delta3, axis = 1)\n",
    "    grad = np.hstack((W1_grad.ravel(), W2_grad.ravel(), b1_grad, b2_grad))\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "\n",
    "def feedforward_autoencoder(theta, hidden_size, visible_size, data):\n",
    "    W1 = theta[0 : hidden_size*visible_size].reshape((hidden_size, visible_size))\n",
    "    b1 = theta[2*hidden_size*visible_size : 2*hidden_size*visible_size+hidden_size].reshape((-1, 1))\n",
    "    a1 = data\n",
    "    z2 = W1.dot(a1) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    \n",
    "    theta = theta.reshape((k, n))\n",
    "\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    \n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    \n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    \n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    \n",
    "    grad = grad.ravel()\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    opt_theta = results['x']\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model\n",
    "\n",
    "def softmax_predict(model, data):\n",
    "    theta = model['opt_theta']\n",
    "    k = model['n_classes']\n",
    "    n = model['input_size']\n",
    "    theta = theta.reshape((k, n))\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Initialise the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 28 * 28\n",
    "n_labels  = 5\n",
    "hidden_size = 200\n",
    "sparsity_param = 0.1\n",
    "\n",
    "lambda_ = 3e-3\n",
    "beta = 3\n",
    "maxiter = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# examples in unlabeled set: 29404\n",
      "# examples in supervised training set: 15298\n",
      "# examples in supervised testing set: 15298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_data   = load_MNIST_images('train-images-idx3-ubyte')\n",
    "mnist_labels = load_MNIST_labels('train-labels-idx1-ubyte')\n",
    "\n",
    "# Simulate a Labeled and Unlabeled set\n",
    "labeled_set   = np.argwhere(mnist_labels < 5).flatten()\n",
    "unlabeled_set = np.argwhere(mnist_labels >= 5).flatten()\n",
    "\n",
    "\n",
    "n_train = int(round(labeled_set.size / 2))\n",
    "train_set = labeled_set[:n_train]\n",
    "test_set  = labeled_set[n_train:]\n",
    "\n",
    "train_data   = mnist_data[:, train_set]\n",
    "train_labels = mnist_labels[train_set]\n",
    "\n",
    "test_data   = mnist_data[:, test_set]\n",
    "test_labels = mnist_labels[test_set]\n",
    "\n",
    "unlabeled_data = mnist_data[:, unlabeled_set]\n",
    "\n",
    "# Output Some Statistics\n",
    "print('examples in unlabeled set: {}'.format(unlabeled_data.shape[1]))\n",
    "print('examples in supervised training set: {}'.format(train_data.shape[1]))\n",
    "print('examples in supervised testing set: {}\\n'.format(test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Sparse autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the results of optimization as following.\n",
      "\n",
      "      fun: 11.662006782289025\n",
      " hess_inv: <314584x314584 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([3.18400423e-07, 1.25577664e-06, 5.84640210e-07, ...,\n",
      "       2.28342563e-05, 2.27904996e-05, 2.28361648e-05])\n",
      "  message: 'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "     nfev: 418\n",
      "      nit: 400\n",
      "   status: 1\n",
      "  success: False\n",
      "        x: array([ 1.06133474e-04,  4.18592214e-04,  1.94880070e-04, ...,\n",
      "       -5.33331390e+00, -5.33398161e+00, -5.33270804e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta = initialize_parameters(hidden_size, input_size)\n",
    "\n",
    "J = lambda theta : sparse_autoencoder_cost(theta, input_size, hidden_size,\n",
    "    lambda_, sparsity_param, beta, unlabeled_data)\n",
    "\n",
    "options = {'maxiter': maxiter, 'disp': True}\n",
    "results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "opt_theta = results['x']\n",
    "\n",
    "print(\"Show the results of optimization as following.\\n\")\n",
    "print(results)\n",
    "\n",
    "# Visualize weights\n",
    "W1 = opt_theta[0:hidden_size*input_size].reshape((hidden_size, input_size))\n",
    "image = display_network(W1.T)\n",
    "plt.figure()\n",
    "plt.imsave('stl_weights.png', image, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Extract Features from the Supervised Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = feedforward_autoencoder(opt_theta, hidden_size, input_size, train_data)\n",
    "test_features  = feedforward_autoencoder(opt_theta, hidden_size, input_size, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1e-4\n",
    "options = {'maxiter': maxiter, 'disp': True}\n",
    "softmax_model = softmax_train(hidden_size, n_labels, lambda_, train_features, train_labels, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy (with learned features): 98.35% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = softmax_predict(softmax_model, test_features)\n",
    "\n",
    "acc = np.mean(test_labels == pred)\n",
    "print(\"The Accuracy (with learned features): {:5.2f}% \\n\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
