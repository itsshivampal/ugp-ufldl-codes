{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST_images(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        n_images = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        rows = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        cols = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        images = np.fromfile(f, dtype=np.ubyte)\n",
    "        images = images.reshape((int(n_images), int(rows * cols)))\n",
    "        images = images.T\n",
    "        images = images.astype(np.float64) / 255\n",
    "        f.close()\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_MNIST_labels(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        n_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        f.close()\n",
    "    return labels\n",
    "\n",
    "    \n",
    "def compute_numerical_gradient(J, theta):\n",
    "    n = theta.size\n",
    "    grad = np.zeros(n)\n",
    "    eps = 1.0e-4\n",
    "    eps2 = 2*eps\n",
    "    for i in range(n):\n",
    "        theta_p = theta.copy()\n",
    "        theta_n = theta.copy()\n",
    "        theta_p[i] = theta[i] + eps\n",
    "        theta_n[i] = theta[i] - eps\n",
    "        grad[i] = (J(theta_p) - J(theta_n)) / eps2\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    \n",
    "    theta = theta.reshape((k, n))\n",
    "\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    \n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    \n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    \n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    \n",
    "    grad = grad.ravel()\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    opt_theta = results['x']\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model\n",
    "\n",
    "def softmax_predict(model, data):\n",
    "    theta = model['opt_theta']\n",
    "    k = model['n_classes']\n",
    "    n = model['input_size']\n",
    "    theta = theta.reshape((k, n))\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "n_classes = 10\n",
    "lambda_ = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_MNIST_images('train-images-idx3-ubyte')\n",
    "labels = load_MNIST_labels('train-labels-idx1-ubyte')\n",
    "input_data = images\n",
    "\n",
    "# Randomly initialise theta\n",
    "theta = 0.005 * np.random.randn(n_classes * input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost, grad = softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)[0]\n",
    "    numgrad = compute_numerical_gradient(J, theta)\n",
    "\n",
    "    n = min(grad.size, 20)\n",
    "    for i in range(n):\n",
    "        print(\"{0:20.12f} {1:20.12f}\".format(numgrad[i], grad[i]))\n",
    "    print('The above two columns you get should be very similar.\\n(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n')\n",
    "\n",
    "    diff = np.linalg.norm(numgrad - grad) / np.linalg.norm(numgrad + grad)\n",
    "    print(\"Norm of difference = \", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'maxiter': 100, 'disp': True}\n",
    "model = softmax_train(input_size, n_classes, lambda_, input_data, labels, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_MNIST_images('train-images-idx3-ubyte')\n",
    "labels = load_MNIST_labels('train-labels-idx1-ubyte')\n",
    "input_data = images\n",
    "\n",
    "pred = softmax_predict(model, input_data)\n",
    "\n",
    "acc = np.mean(labels == pred)\n",
    "print(\"Accuracy: {:5.2f}% \\n\".format(acc*100))\n",
    "\n",
    "# we get 93.13% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
